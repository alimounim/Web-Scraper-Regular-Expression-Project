{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94511fbe-eb31-44d5-8b41-f3a37d6131d0",
   "metadata": {},
   "source": [
    "### **Web Scraping and Text Analysis of Martin Luther King Jr.'s Speech**\n",
    "\n",
    "This project involves web scraping, text preprocessing, and word frequency analysis of Martin Luther King Jr.'s speech from an online source. The script leverages Python libraries such as **BeautifulSoup**, **Requests**, **re** (Regular Expressions), and **pandas** to extract, clean, and analyze textual data.\n",
    "\n",
    "#### **Project Workflow:**\n",
    "\n",
    "1. **Web Scraping:**  \n",
    "   - The script fetches the HTML content of the webpage containing the speech using the `requests` library.  \n",
    "   - It parses the HTML using `BeautifulSoup` and extracts all paragraphs (`<p>` tags) that contain the speech text.\n",
    "\n",
    "2. **Text Preprocessing:**  \n",
    "   - The extracted text is combined into a single string for uniform analysis.  \n",
    "   - Carriage returns and newline characters are replaced with spaces.  \n",
    "   - Punctuation is removed using regular expressions.  \n",
    "   - The text is converted to lowercase to ensure consistency.  \n",
    "   - The cleaned text is split into individual words.\n",
    "\n",
    "3. **Word Frequency Analysis:**  \n",
    "   - The words are organized into a `pandas` DataFrame.  \n",
    "   - The frequency of each unique word is calculated and stored.\n",
    "\n",
    "4. **Exporting Results:**  \n",
    "   - The word frequency data is exported to a CSV file (`mlk_speech_counts.csv`) with two columns: `Word` and `Counts`.\n",
    "\n",
    "#### **Purpose of the Project:**  \n",
    "- **Data Collection:** Demonstrates the use of web scraping techniques to extract unstructured text data from a webpage.  \n",
    "- **Text Cleaning:** Highlights preprocessing steps to prepare raw text for analysis.  \n",
    "- **Data Analysis:** Provides insights into the most frequently used words in the speech.  \n",
    "- **Data Export:** Stores the results in a structured format (CSV) for further exploration and visualization.\n",
    "\n",
    "\n",
    "#### **Webpage Link:**\n",
    "- ****\n",
    "This project serves as a foundational example of how web scraping, text processing, and data analysis can be integrated into a cohesive workflow using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39fdc0c4-9036-43f7-97b7-6ee40e64fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the BeautifulSoup class from the bs4 library for web scraping and HTML parsing.\n",
    "from bs4 import BeautifulSoup  \n",
    "\n",
    "# Importing the requests library to make HTTP requests to fetch web page content.\n",
    "import requests  \n",
    "\n",
    "# Importing the re module for regular expression operations, useful for pattern matching in text.\n",
    "import re  \n",
    "\n",
    "# Importing the pandas library to work with data in a tabular format, such as dataframes.\n",
    "import pandas as pd  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "290fda00-4285-46f5-930f-dc2bf5536cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the webpage containing the text of Martin Luther King Jr.'s speech.\n",
    "url = 'http://analytictech.com/mb021/mlk.htm'\n",
    "\n",
    "# Send an HTTP GET request to the specified URL to retrieve the page content.\n",
    "page = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page using BeautifulSoup.\n",
    "soup = BeautifulSoup(page.text, 'html')\n",
    "\n",
    "# Extract all paragraphs (<p>) from the HTML document.\n",
    "mlkj_speech = soup.find_all('p')\n",
    "\n",
    "# Extract the text content from each paragraph tag and store it in a list.\n",
    "speech_combined = [p.text for p in mlkj_speech]\n",
    "\n",
    "# Combine all paragraph texts into a single string, separating each paragraph with a space.\n",
    "string_speech = ' '.join(speech_combined)\n",
    "\n",
    "# Display the first 100 characters of the speech string (for preview purposes).\n",
    "string_speech[:100]\n",
    "\n",
    "# Clean the string by replacing any carriage return and newline characters with a space.\n",
    "string_speech_cleaned = string_speech.replace('\\r\\n', ' ')\n",
    "\n",
    "# Remove all punctuation from the string using a regular expression.\n",
    "speach_no_punt = re.sub(r'[^\\w\\s]', '', string_speech_cleaned)\n",
    "\n",
    "# Convert the cleaned string into lowercase for uniformity.\n",
    "speech_lower = speach_no_punt.lower()\n",
    "\n",
    "# Split the lowercase string into individual words using whitespace as the delimiter.\n",
    "speech_broken_out = re.split(r'\\s+', speech_lower)\n",
    "\n",
    "# Create a pandas DataFrame from the list of words and count the frequency of each unique word.\n",
    "df = pd.DataFrame(speech_broken_out).value_counts()\n",
    "\n",
    "# Export the word frequency data to a CSV file with the word as the index and its count as a column.\n",
    "df.to_csv(r'Web Scraper + Regular Expression Project/mlk_speech_counts.csv', header=['Counts'], index_label='Word')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
